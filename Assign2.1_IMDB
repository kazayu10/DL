import numpy as np 
import pandas as pd 
import tensorflow as tf
import string
import re
import os
import nltk
from nltk.corpus import stopwords, twitter_samples
from nltk.stem import PorterStemmer
from nltk.tokenize import TweetTokenizer
import warnings
warnings.filterwarnings('ignore')

imdb_data = pd.read_csv("IMDB_Dataset.csv")

# Converting the positive labels to 1 and the negative labels to 0
imdb_data['sentiment'].mask(imdb_data['sentiment'] == 'positive', 1, inplace=True)
imdb_data['sentiment'].mask(imdb_data['sentiment'] == 'negative', 0, inplace=True)

# Get the reviews and the labels
all_reviews = list(imdb_data['review'])
labels = np.asarray(imdb_data['sentiment'])

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
# cutoff reviews after 200 words
maxlen = 200
training_samples = 40000
validation_samples = 5000
testing_samples = 5000
# consider the top 100000 words in the dataset
max_words = 10000
# tokenize each review in the dataset
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(all_reviews)
sequences = tokenizer.texts_to_sequences(all_reviews)

word_index = tokenizer.word_index
print("Found {} unique tokens.".format(len(word_index)))
ind2word = dict([(value, key) for (key, value) in word_index.items()])

# pad the sequences so that all sequences are of the same size
data = pad_sequences(sequences, maxlen=maxlen)

# shuffling the data and labels
indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]

# Splitting the data set to training and validation datasets 
x_train = data[: training_samples]
y_train = labels[: training_samples]
x_val = data[training_samples : training_samples + validation_samples]
y_val = labels[training_samples : training_samples + validation_samples]
x_test = data[training_samples + validation_samples: training_samples + validation_samples + testing_samples]
y_test = labels[training_samples + validation_samples: training_samples + validation_samples + testing_samples]
x_train = np.asarray(x_train).astype(np.int)
y_train = np.asarray(y_train).astype(np.int)
x_val = np.asarray(x_val).astype(np.int)
y_val = np.asarray(y_val).astype(np.int)
x_test = np.asarray(x_test).astype(np.int)
y_test = np.asarray(y_test).astype(np.int)

x_train.shape

x_val.shape

x_test.shape

#Simple Model
embedding_dim = 300
simple_model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(max_words, embedding_dim, input_length=maxlen),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),

    tf.keras.layers.Dense(1, activation='sigmoid')
])
simple_model.compile(loss='binary_crossentropy',
                     optimizer='adam',
                     metrics=['accuracy'])
            
simple_model_history = simple_model.fit(x_train,y_train,
                                        validation_data=(x_val,y_val),
                                        epochs=10)
                                        
embedding_dim = 300
lstm_model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(max_words, embedding_dim, input_length=maxlen),
    tf.keras.layers.LSTM(units=64, activation='tanh', return_sequences=True),
    tf.keras.layers.LSTM(units=32, activation='tanh'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
lstm_model.compile(loss='binary_crossentropy',
                   optimizer='adam',
                   metrics=['accuracy'])
                   
lstm_model_history = lstm_model.fit(x_train,y_train,
                                    validation_data=(x_val,y_val),
                                    epochs=7)
                                    
embedding_dim = 300
gru_model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(max_words, embedding_dim, input_length=maxlen),
    tf.keras.layers.GRU(units=64, activation='tanh', return_sequences=True),
    tf.keras.layers.GRU(units=32, activation='tanh'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
gru_model.compile(loss='binary_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])
                  
gru_model_history = gru_model.fit(x_train,y_train,
                                  validation_data=(x_val,y_val),
                                  epochs=7)
                                  
def eval_model(model):
    model_acc_train_dataset = model.evaluate(x_train, y_train)
    model_acc_val_dataset = model.evaluate(x_val, y_val)
    model_acc_test_dataset = model.evaluate(x_test, y_test)
    return model_acc_train_dataset, model_acc_val_dataset, model_acc_test_dataset
    
simple_model_acc_train_dataset, simple_model_acc_val_dataset, simple_model_acc_test_dataset = eval_model(simple_model)
gru_model_acc_train_dataset, gru_model_acc_val_dataset, gru_model_acc_test_dataset = eval_model(gru_model)
lstm_model_acc_train_dataset, lstm_model_acc_val_dataset, lstm_model_acc_test_dataset = eval_model(lstm_model)

train_accs = [simple_model_acc_train_dataset[1], gru_model_acc_train_dataset[1],
              lstm_model_acc_train_dataset[1]]
val_accs = [simple_model_acc_val_dataset[1], gru_model_acc_val_dataset[1],
            lstm_model_acc_val_dataset[1]]
test_accs = [simple_model_acc_test_dataset[1], gru_model_acc_test_dataset[1],
             lstm_model_acc_test_dataset[1]]
             
models_eval_df = pd.DataFrame({"Training Accuracy":train_accs, "Validation Accuracy":val_accs, "Testing Accuracy":test_accs},
                              index=['simple_model', 'gru_model','lstm_model'])
                              
models_eval_df
